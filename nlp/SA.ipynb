{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分析："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week1 情感分析：\n",
    "\n",
    "* 情感分析主要目标：对文本中评论者的情感**sentiments**，情绪**emotions**以及观点**opinions**进行提取，并将这些信息在一些领域中。例如：消费者对产品的评价，观众对电影的评价，预测市场趋势或选举结果等。\n",
    "\n",
    "\n",
    "* 情感分析主要任务：将非结构化的数据，例如：用户的观点，自动地转换为结构化的数据 $\\left(o_j ,f_{jk}, so_{ijkl}, h_i, t_l\\right)$。**需要注意的是：情感分析主要关注评论者的主观声明，而非一些事实性的声明。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subjectivity Classification："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主观臆断分类通常是情感分析的第一步。\n",
    "\n",
    "* 主观 subjective: it is such a nice phone.\n",
    "\n",
    "* 客观 objective: I bought an iphone a few days ago.\n",
    "\n",
    "然而，主观句并不一定揭露评论者的情感或观点，客观句有可能会间接地阐明评论者的观点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. quintuple $\\left(o_j ,f_{jk}, so_{ijkl}, h_i, t_l\\right)$：\n",
    "\n",
    "* $o_j$：target object 目标对象。\n",
    "\n",
    "\n",
    "* $f_{jk}$：features 目标对象$o_j$的特征。\n",
    "\n",
    "\n",
    "* $so_{ijkl}$：sentiment opinions 观点持有者$h_i$在时刻$t_l$下对于特征$f_{jk}$的情感观点。\n",
    "\n",
    "\n",
    "* $h_i$：opinion holder 观点持有者。\n",
    "\n",
    "\n",
    "* $t_l$：time 时间。\n",
    "\n",
    "下面为某一顾客的评论：\n",
    "\n",
    "<img src=\"TP_github/example_SA_1.png\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $o_j$: iphone.\n",
    "\n",
    "\n",
    "* $f_{jk}$: phone, touch screen, voice quality, battery life, prize.\n",
    "\n",
    "\n",
    "* $so_{ijkl}$: positive, positive, positive, negative, negative.\n",
    "\n",
    "\n",
    "* $h_i$: I, I, I, I, mother.\n",
    "\n",
    "\n",
    "* $t_i$: post date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 粒度等级 Granularity level："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Document level: 依据整个文本，例如：评论，分析情感。\n",
    "\n",
    "\n",
    "* Sentence level: 依据主观句分析情感。\n",
    "\n",
    "\n",
    "* Feature level: 对不同的特征进行情感分析。\n",
    "\n",
    "例如，feature level:\n",
    "\n",
    "feature1 touch screen:\n",
    "\n",
    "* positive: The touch screen was really cool.\n",
    "\n",
    "* positive: The touch screen was so easy to use and can do amazing things.\n",
    "\n",
    "* negative: The screen is easily scratched.\n",
    "\n",
    "* negative: I have a lot of difficulty in removing finger marks from the touch screen.\n",
    "\n",
    "feature2 battery life:\n",
    "\n",
    "* ...\n",
    "\n",
    "**在特征等级下，将数据视觉化：**\n",
    "\n",
    "* 对某一目标所有的特征进行总结根据不同的用户评论或者对不同目标相同特征进行对比：\n",
    "<img src=\"TP_github/SA_comparison.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "* 计算出不同特征的好评率：\n",
    "<img src=\"TP_github/SA_features.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "* 计算出某一目标在某一时间段内的好评率的变化：\n",
    "<img src=\"TP_github/SA_timetrend.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 情感分析面临的挑战："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Named Entity Recognition: 识别目标对象。例如：地名，公司名等。\n",
    "\n",
    "\n",
    "* Information Extraction\n",
    "\n",
    "\n",
    "* Sentiment determination\n",
    "\n",
    "\n",
    "* Metadata Extraction\n",
    "\n",
    "\n",
    "* synonym match: 同义词匹配。\n",
    "\n",
    "\n",
    "* Co-reference resolution: 指代消歧，代词指代问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week2 情感分析方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基于词汇表的方法 Lexicon-based:\n",
    "\n",
    "### 1.1 基本概念及原理：\n",
    "\n",
    "通过使用包含情感词汇，例如：good, bad , great, etc，的词汇表。\n",
    "\n",
    "* rule-based subjectivity classifier: 判断句子或文本是否是主观的。如果一个句子或文本包含至少两个情感性的词汇，此句子或文本为主观的，否则，为客观的。\n",
    "\n",
    "\n",
    "* rule-based sentiment classifier (subjective sentence or document): 统计句子或文本中积极以及消极的词汇出现的次数。如果，积极词汇的数量大于消极词汇的数量，则此句子或文本的情感为积极的，反之，为消极的。\n",
    "\n",
    "\n",
    "* rule-based sentiment classifier (features): 与主观句和文本情感分析类似，只不过在统计情感词数量之前，特征情感分类需要通过信息提取技术获得所有的特征。\n",
    "    \n",
    "简单的流程：输入(feature, sentence)格式。统计情感词汇次数，输出 1, -1, 0**（binary）**。**一些复杂的情感词汇的统计，考虑到程度副词，否定，以及一些基于文本或独立于文本的情感词汇。**\n",
    "\n",
    "例如：\n",
    "\n",
    "<img src=\"TP_github/lexicon.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "输出仅为positive, negative, neutral并不能准确表达情感的强烈程度。因此，使用一定范围的数字表达情感**（gradable）**，可以使结果更加准确。\n",
    "\n",
    "例如：\n",
    "<img src=\"TP_github/adv_degree.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "除此之外，一些其他规则可以使结果更加准确，**例如：negation rule, capitalization rule**。\n",
    "\n",
    "* negation rule：如果情感词左右有否定词出现，将情感词的权重减一，然后取反。\n",
    "    例如：I'm not good today.\n",
    "\n",
    "\n",
    "* capitalization rule：对于大小写敏感。例如：I'm GOOD today. I'm AWFUL today.\n",
    "\n",
    "\n",
    "* intensifier rule：如果副词对应的情感词为positive，权重为副词权重**加**情感词权重。反之，权重为副词权重减情感词权重。例如：I'm feeling very good.\n",
    "\n",
    "\n",
    "* diminisher rule：例如：somewhat, barely, rarely。如果副词对应的情感词为positive，权重为副词权重**减**情感词权重.反之，权重为副词权重加情感词权重。例如：I'm somewhat good.\n",
    "\n",
    "\n",
    "* Exclamation rule：与intensifier rule 类似。例如：Great show!!!. Weight(!!!) = 2\n",
    "\n",
    "\n",
    "* Emoticon rule：表情 😊，相当于情感词。\n",
    "\n",
    "### 1.2 基于词汇表方法的优缺点：\n",
    "\n",
    "优点：\n",
    "* 适用于各种形式的文本。论坛，博客等\n",
    "* 语言独立。\n",
    "* 不需要数据训练。\n",
    "* 易扩展。\n",
    "\n",
    "缺点：\n",
    "* 需要情感词汇表，此词汇表应该是全面的，包含缩写，错误拼写等。\n",
    "\n",
    "**因此，此方法重点在于如何获得一个全面的词汇表数据集。**\n",
    "\n",
    "Manually, Dictionary-based (WordNet), Corpus-based.\n",
    "\n",
    "### 1.3 手动创建的语料库:\n",
    "\n",
    "* SentiWordNet: WordNet是近义词辞典。sentiWordNet是其一版本包含 positivity, negativity, objectivity的情感得分。\n",
    "<img src=\"TP_github/sentiwordnet.png\" width=\"600\" height=\"400\">\n",
    "* Linguistic Inquiry and Word Count (LIWC) lexicon: \n",
    "<img src=\"TP_github/liwc.png\" width=\"600\" height=\"400\">\n",
    "* General Inquirer: 包含多种情感类别。\n",
    "<img src=\"TP_github/general_inquirer.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "### 1.4 半自动创建的语料库：\n",
    "\n",
    "Dictionary-based (WordNet), Corpus-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week3 基于语料库的方法 Corpus-based:\n",
    "\n",
    "\n",
    "使用监督学习，将标注过的语料库作为训练集训练分类器。语料库可以手动创建，也可以通过一些网站获得。例如：Amazon Product Reviews (1-5\n",
    "stars), Rotten Tomatoes, complaints.com, bitterlemons.com\n",
    "\n",
    "* 首先使用subjective classifier识别及消除主观句。\n",
    "\n",
    "* 将剩余数据用于训练分类器，例如：朴素贝叶斯。\n",
    "\n",
    "* 多元伯努利事件模型\n",
    "\n",
    "* 多项式事件模型\n",
    "\n",
    "## 1. 朴素贝叶斯：\n",
    "\n",
    "贝叶斯公式：\n",
    "\n",
    "$$\n",
    "\\text{P}(c_i|\\text{T})=\\frac{\\text{P}(\\text{T}|c_i)\\text{P}(c_i)}{\\text{P}(\\text{T})}\n",
    "$$\n",
    "\n",
    "假设特征之间相互独立：\n",
    "$$\n",
    "\\text{P}(\\text{T}|c_i) = \\text{P}(\\text{t}_1, \\text{t}_2, \\dots, \\text{t}_j|c_i) \\approx \\prod_{j=1}^n \\text{P}(\\text{t}_j|c_i)\n",
    "$$\n",
    "\n",
    "目标：\n",
    "$$\n",
    "\\text{argmax} \\text{P}(c_i) \\prod_{j=1}^n \\text{P}(\\text{t}_j|c_i)\n",
    "$$\n",
    "\n",
    "* prior : $\\text{P}(c_i)$\n",
    "\n",
    "\n",
    "* likelihood : $\\text{P}(\\text{T}|c_i)$\n",
    "\n",
    "\n",
    "* evidence : $\\text{P}(\\text{T})$\n",
    "\n",
    "举例：\n",
    "\n",
    "<img src=\"TP_github/naivebayesian.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "<img src=\"TP_github/NB1.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "先验概率：\n",
    "\n",
    "$ \\text{P(positive)} = \\frac{3}{7} $\n",
    "\n",
    "$ \\text{P(negative)} = \\frac{4}{7} $\n",
    "\n",
    "可能性：\n",
    "\n",
    "$\\text{P}(\\text{fantastic|positive}) = \\frac{1}{10}$\n",
    "$\\text{P}(\\text{good|positive}) = \\frac{1}{10}$\n",
    "$\\text{P}(\\text{lovely|positive}) = \\frac{1}{10}$\n",
    "\n",
    "$\\text{P}(\\text{fantastic|negative}) = \\frac{0}{8}$\n",
    "$\\text{P}(\\text{good|negative}) = \\frac{0}{8}$\n",
    "$\\text{P}(\\text{lovely|negative}) = \\frac{0}{8}$\n",
    "\n",
    "后验概率：\n",
    "\n",
    "$\\text{P}(\\text{doc8|positive}) = \\frac{3}{7} \\times \\frac{1}{10}\\times\\frac{1}{10}\\times\\frac{1}{10}=0.00043$\n",
    "\n",
    "$\\text{P}(\\text{doc8|negative}) = \\frac{4}{7}\\times\\frac{0}{8}\\times\\frac{0}{8}\\times\\frac{0}{8}=0$\n",
    "\n",
    "所以，doc8的情感为positive。\n",
    "\n",
    "**由于数据集并不全面，因此某一单词的概率为零会导致整个文本概率为0。拉普拉斯光滑可以解决此问题，公式如下所示。**\n",
    "\n",
    "$$\n",
    "\\text{P}(t_j|c_i) = \\frac{\\text{count}(t_j, c_j)+1}{\\text{count}(c_j)+\\text{|V|}}\n",
    "$$\n",
    "\n",
    "V为特征数量（单词的数量）。\n",
    "\n",
    "**总结：**\n",
    "\n",
    "* 当训练数据充足的情况下，朴素贝叶斯是一种简单且高效的算法。\n",
    "\n",
    "\n",
    "* 当样本空间的数据不平衡的情况下，例如：positive样本数量远大于negative数量，先验概率尤为重要。\n",
    "\n",
    "\n",
    "* 对于多分类任务可以使用逻辑回归，或多次使用贝叶斯分类器。例如：3分类任务，将某一类作为positive其余两类作为negative。\n",
    "\n",
    "## 2. 评估分类模型：\n",
    "\n",
    "### 2.1 混淆矩阵：\n",
    "\n",
    "<img src=\"TP_github/table_evaluation.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "1. Accuracy: 分类正确的概率。\n",
    "$$ \\frac{\\text{TN+TP}}{\\text{TN+TP+FP+FN}}$$\n",
    "\n",
    "\n",
    "2. Precision: 预测为正或负的样本中正确的概率。\n",
    "$$ \\frac{\\text{TP}}{\\text{TP+FP}} or \\frac{\\text{TN}}{\\text{TN+FN}} $$\n",
    "\n",
    "\n",
    "3. Recall: 真实标签为正或负的样本中正确的概率。\n",
    "$$ \\frac{\\text{TP}}{\\text{TP+FN}} or \\frac{\\text{TN}}{\\text{TN+FP}} $$\n",
    "\n",
    "\n",
    "4. F-measure: precision与recall加权调和平均。\n",
    "$$ \\frac{2\\text{PrecisionRecall}}{\\text{Precision + Recall}} $$\n",
    "\n",
    "### 2.2 ROC曲线：\n",
    "\n",
    "ROC接收者操作特征曲线：\n",
    "<img src=\"TP_github/ROC.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "* 横坐标为（fpr）：$\\frac{\\text{FP}}{\\text{FP+TN}}$，预测为正但实际为负的样本占所有负例样本的比例。\n",
    "\n",
    "\n",
    "* 纵坐标为（tpr）：$\\frac{\\text{TP}}{\\text{TP+FN}}$，预测为正且实际为正的样本占所有正例样本的比例。\n",
    "\n",
    "\n",
    "* AUC ：曲线下的面积。越接近于1代表模型预测越准确。\n",
    "\n",
    "1. ROC曲线能很容易的查出任意阈值对学习器的泛化性能影响。\n",
    "\n",
    "2. 有助于选择最佳的阈值。ROC曲线越靠近左上角，模型的查全率就越高。最靠近左上角的ROC曲线上的点是分类错误最少的最好阈值，其假正例和假反例总数最少。\n",
    "\n",
    "3. 可以对不同的学习器比较性能。将各个学习器的ROC曲线绘制到同一坐标中，直观地鉴别优劣，靠近左上角的ROC曲所代表的学习器准确性最高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
