{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前馈神经网络 feedforward neuronal network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 感知机算法：\n",
    "\n",
    "> 通过一条直线对数据进行分类，但是无法处理非线性问题。（具体参考 week2 term1 ml）\n",
    "\n",
    "<img src=\"NLP_github/perceptron.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 多层感知机：\n",
    "\n",
    "> 多层感知机也称之为前馈神经网络。**（全联接网络）注意：只有对隐含层的输出使用激活函数才可以处理非线性问题。**\n",
    "\n",
    "<img src=\"NLP_github/nn.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 前向传播与反向传播：\n",
    "\n",
    "### 3.1 前向传播：\n",
    "\n",
    "<img src=\"NLP_github/forward.jpeg\" width=\"500\" height=\"500\">\n",
    "\n",
    "### 3.2 反向传播以及更新：\n",
    "\n",
    "变量定义如下：\n",
    "<img src=\"NLP_github/definition.jpeg\" width=\"500\" height=\"500\">\n",
    "\n",
    "> 1. 链式求导，如下图所示：\n",
    "\n",
    "<img src=\"NLP_github/chain.jpeg\" width=\"500\" height=\"500\">\n",
    "\n",
    "> 根据链式求导，对权重进行更新。\n",
    "\n",
    "<img src=\"NLP_github/bp_example.jpeg\" width=\"500\" height=\"500\">\n",
    "\n",
    "> 对上述链式求导过程表达成公式。**总的loss对某个权重的偏导为 $\\frac{\\partial C}{\\partial W_{jk}^l} = a_k^{l-1} \\delta_j^l$。其中，$a_k^{l-1}$表示第l-1层的第k个神经元的输出。**\n",
    "\n",
    "> 使用梯度下降对权重进行更新：$w = w - \\eta \\frac{\\delta \\text{E}_{total}}{\\delta w}$。\n",
    "\n",
    "\n",
    "* 激活函数求导，以sigmoid函数为例 $ \\text{S}^\\prime (x) = \\text{S}(x) \\times (1 - \\text{S}(x))$。具体推导步骤如下：\n",
    "\n",
    "$$\n",
    "\\text{S}(x) = \\frac{1}{1 + \\text{e}^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\text{S}(x)} = 1 + \\text{e}^{-x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f^\\prime(x) = - \\frac{\\text{S}^\\prime(x)}{\\text{S}^2(x)}，等式1\n",
    "$$\n",
    "\n",
    "$$\n",
    "f^\\prime(x) = (1+\\text{e}^{-x})^\\prime = - \\text{e}^{-x} = 1 - f(x) = 1 - \\frac{1}{\\text{S}(x)} = \\frac{\\text{S}(x)-1}{\\text{S}(x)}，等式2\n",
    "$$\n",
    "$$\n",
    "根据等式1，2可得：- \\frac{\\text{S}^\\prime(x)}{\\text{S}^2(x)} = \\frac{\\text{S}(x) - 1}{\\text{S}(x)}\n",
    "$$\n",
    "\n",
    "* 第l层产生的错误，$\\delta^l = \\left((w^{l+1})^\\text{T} \\delta^{l+1}) \\sigma^\\prime (z^l) \\right)$。其中，$z_j^l$表示第l层，第j个神经元的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 前向传播反向传播计算过程：\n",
    "\n",
    "<img src=\"NLP_github/steps_0.jpeg\" width=\"700\" height=\"500\">\n",
    "<img src=\"NLP_github/steps_1.jpeg\" width=\"700\" height=\"500\">\n",
    "<img src=\"NLP_github/steps_2.jpeg\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 防止过拟合：\n",
    "\n",
    "* **L2-regularisation：in the weights of each layer (added in the loss function of each layer)**\n",
    "\n",
    "\n",
    "* **Dropout：randomly ignore a percentage (e.g. 20% or 50%) of layer outputs during training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. NLP应用：\n",
    "\n",
    "> word2vec Skip-gram model & Continuous BOW (CBOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
