{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bert: Bi-directional Encoder Representations from Transformer\n",
    "\n",
    "可以用来获得包含上下文语境的 word embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 self-attention:\n",
    "\n",
    "一个序列每个字符对其上下文字符的影响作用都不同，每个字对序列的语义信息贡献也不同，可以通过一种机制将原输入序列中字符向量通过加权融合序列中所有字符的语义向量信息来产生新的向量，即增强了原语义信息。如下图所示，一个句子中计算每个单词（key）与目标单词（query）的相似度作为权重。通过权重加权融合目标字的Value向量和各个上下文字的Value向量，作为Attention的输出，即：目标字的增强语义向量表示。\n",
    "\n",
    "<img src=\"NLP_github/self_attention_2.png\" width=\"500\">\n",
    "\n",
    "具体过程如下：\n",
    ">\n",
    ">1. 每个自注意力模型对每个单词需要创建三个向量query，key，value，通过三个参数矩阵。\n",
    "><img src=\"NLP_github/qkv.png\" width=\"500\">\n",
    ">\n",
    ">2. 计算每个query与每个key的得分，然后通过softMax函数归一化，其中score分数开根号处理，由于小的score会导致softmax函数更大的梯度。\n",
    "><img src=\"NLP_github/qkv_1.png\" width=\"500\">\n",
    ">\n",
    ">3. 对于value向量加权求和表示目标单词的向量。\n",
    "><img src=\"NLP_github/qkv_2.png\" width=\"500\">\n",
    "\n",
    "## 1.2 Multi-head self-attention:\n",
    "\n",
    "为了增强Attention的多样性，进一步利用不同的Self-Attention模块获得文本中每个字在不同语义空间下的增强语义向量，并将每个字的多个增强语义向量进行线性组合，从而获得一个最终的与原始字向量长度相同的增强语义向量，如下图所示。\n",
    "\n",
    "<img src=\"NLP_github/self_attention_3.png\" width=\"600\">\n",
    "\n",
    "具体过程如下：\n",
    ">1. 每个注意力产生不同的输出矩阵（单词向量构成的矩阵用z表示）。\n",
    ">\n",
    "><img src=\"NLP_github/multi_head.png\" width=\"600\">\n",
    ">\n",
    ">2. 将这些矩阵拼接到一起，与参数矩阵相乘得到最后的输出。\n",
    ">\n",
    "><img src=\"NLP_github/mh_1.png\" width=\"600\">\n",
    "><img src=\"NLP_github/mh_2.png\" width=\"600\">\n",
    ">\n",
    ">下图中不同的颜色表示不同的注意力所关注的单词，例如：橙色的注意力将信息关注于\"The\"，\"animal\"，绿色的注意力将信息关注于\"tired\"。\n",
    "><img src=\"NLP_github/mh_3.png\" width=\"500\">\n",
    "\n",
    "## 1.3 Transformer:\n",
    "\n",
    "* 残差连接：将模块的输入与输出直接相加，作为最后的输出。\n",
    "* 线性转换：对每个字的增强语义向量再做两次线性变换，以增强整个模型的表达能力。这里，变换后的向量与原向量保持长度相同。\n",
    "\n",
    "<img src=\"NLP_github/transformer.png\" width=\"600\">\n",
    "\n",
    "## 1.4 Bert:\n",
    "\n",
    "把Transformer Encoder模块一层一层的堆叠起来就是BERT。\n",
    "\n",
    "<img src=\"NLP_github/Bert.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transformer: https://zhuanlan.zhihu.com/p/54523019\n",
    "\n",
    "* transformer: https://msd.misuland.com/pd/2884250068896976500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
