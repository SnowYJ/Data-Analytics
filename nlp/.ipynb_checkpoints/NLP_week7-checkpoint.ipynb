{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型 - 神经网络:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN\n",
    "* LSTM\n",
    "* GRU\n",
    "* Bi-directional RNN\n",
    "* RNN + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 循环神经网络 RNN："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN是一种语言模型（给定一个一句话前面的部分，预测接下来最有可能的一个词是什么）可以捕获文本内单词之间的依存关系。在RNN的文本分类模型中，可以把RNN看成一个encoder，将需要被分类的文本表示为一个dense vector，再使用全连接层与softmax输出各类别的概率。\n",
    "\n",
    "<img src=\"NLP_github/RNN_1.png\" width=\"600\" height=\"500\">\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td ><center><img src=\"NLP_github/rnn_2.png\" width=\"150\"></center></td>\n",
    "        <td ><center><img src=\"https://pic2.zhimg.com/v2-017335693892e081fa6d0e678d4a634d_b.gif\" width=\"300\"></center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "注意：普通RNN内部并非为多层网络，仅仅是一个tanh层，输入为每个单词的one-hot（与前馈神经网络相同）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述的RNN通过时序方式进行排列即为如下形式，每一个x代表同一文章内的一个单词。\n",
    "\n",
    "<img src=\"NLP_github/RNN.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "其中，U代表输入层到隐含层的参数矩阵，V代表隐含层到输出层的参数矩阵，W代表循环层的参数矩阵。由上图可知，时刻t隐含层的输出与不仅与当前时刻的输入x有关，而且与t-1时刻隐含层的输出有关。因此，某一时刻隐含层的输出可表示为如下形式：\n",
    "\n",
    "$$\n",
    "s_n = f (\\text{W} s_{n-1} + \\text{U} x_n)\n",
    "$$\n",
    "\n",
    "**RNN类似于HMM，将之前的信息考虑进去，预测值之后的信息。但是，RNN受限于短期记忆问题。如果一个序列足够长，那它们很难把信息从较早的时间步传输到后面的时间步。因此，如果你尝试处理一段文本来进行预测，RNN可能在开始时就会遗漏重要信息。在反向传播过程中，RNN中存在梯度消失问题。梯度是用于更新神经网络权重的值，梯度消失问题是指随着时间推移，梯度在传播时会下降，如果梯度值变得非常小，则不会继续学习。解决办法，LSTM长短期记忆网络或GRU。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 循环神经网络参数更新 - BPTT："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 前向传播：\n",
    "\n",
    "$$\n",
    "s_n = f (\\text{W} s_{n-1} + \\text{U} x_n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "            \\begin{bmatrix}\n",
    "                s_1^t  \\\\\n",
    "                s_2^t  \\\\\n",
    "                \\vdots\\\\ \n",
    "                s_n^t\n",
    "            \\end{bmatrix}\n",
    "\\end{align*} = \n",
    "f \\left( \\begin{bmatrix}\n",
    "                w_{11} & w_{12} & \\dots & w_{1m}\\\\\n",
    "                w_{21} & w_{22} & \\dots & w_{2m}\\\\\n",
    "                \\vdots\\\\ \n",
    "                w_{n1} & w_{n2} & \\dots & w_{nm}\\\\\n",
    "\\end{bmatrix} \\times \n",
    "\\begin{bmatrix}\n",
    "                s_1^{t-1}  \\\\\n",
    "                s_2^{t-1}  \\\\\n",
    "                \\vdots\\\\ \n",
    "                s_n^{t-1}\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "                u_{11} & u_{12} & \\dots & u_{1m}\\\\\n",
    "                u_{21} & u_{22} & \\dots & u_{2m}\\\\\n",
    "                \\vdots\\\\ \n",
    "                u_{n1} & u_{n2} & \\dots & u_{nm}\\\\\n",
    "\\end{bmatrix} \\times\n",
    "\\begin{bmatrix}\n",
    "                x_1  \\\\\n",
    "                x_2  \\\\\n",
    "                \\vdots\\\\ \n",
    "                x_n\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "* 反向传播：误差沿两个方向传播，U（沿时间序列方向） 和 W（沿循环层方向）。\n",
    "\n",
    ">**首先，更新U：**用 $\\text{net}_t$ 表示隐含层神经元在时刻t的**输入**：$net_t = \\text{W} s_{t-1} + \\text{U} x_t, s_{t-1} = f(\\text{net}_{t-1})$\n",
    ">\n",
    ">$$\n",
    "\\frac{\\partial \\text{net}_t}{\\partial \\text{net}_{t-1}} = \\frac{\\partial \\text{net}_t}{\\partial s_{t-1}} \\frac{\\partial s_{t-1}}{\\partial \\text{net}_{t-1}}\n",
    "$$\n",
    ">\n",
    ">\n",
    "> $$\\frac{\\partial \\text{net}_t}{\\partial s_{t-1}}  = \\text{W},  \\frac{\\partial s_{t-1}}{\\partial \\text{net}_{t-1}} = diag(f^\\prime(net^{t-1})) = \n",
    "\\begin{bmatrix}\n",
    "                f^\\prime(net^{t-1}_1) & \\dots & 0 \\\\\n",
    "                0 & f^\\prime(net^{t-1}_2) & \\dots \\\\\n",
    "                \\vdots & \\vdots & \\vdots \\\\ \n",
    "                0 & \\dots & f^\\prime(net^{t-1}_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    ">\n",
    ">$$\n",
    "\\frac{\\partial \\text{net}_t}{\\partial \\text{net}_{t-1}} = \\begin{bmatrix}\n",
    "                w_{11} & w_{12} & \\dots & w_{1m}\\\\\n",
    "                w_{21} & w_{22} & \\dots & w_{2m}\\\\\n",
    "                \\vdots\\\\ \n",
    "                w_{n1} & w_{n2} & \\dots & w_{nm}\\\\\n",
    "\\end{bmatrix} \\times  \n",
    "\\begin{bmatrix}\n",
    "                f^\\prime(net^{t-1}_1) & \\dots & 0 \\\\\n",
    "                0 & f^\\prime(net^{t-1}_2) & \\dots \\\\\n",
    "                \\vdots & \\vdots & \\vdots \\\\ \n",
    "                0 & \\dots & f^\\prime(net^{t-1}_n)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "                w_{11} f^\\prime(net^{t-1}_1) & w_{12} f^\\prime(net^{t-1}_2) & \\dots & w_{1n} f^\\prime(net^{t-1}_n) \\\\\n",
    "                w_{21} f^\\prime(net^{t-1}_1) & w_{22} f^\\prime(net^{t-1}_2) & \\dots & w_{2n} f^\\prime(net^{t-1}_n)\\\\\n",
    "                \\vdots & \\vdots & \\vdots \\\\ \n",
    "                w_{n1} f^\\prime(net^{t-1}_1) & w_{n2} f^\\prime(net^{t-1}_2) & \\dots & w_{nn} f^\\prime(net^{t-1}_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ">\n",
    ">下面就是将误差项沿时间反向传播的算法（更新W）：\n",
    ">\n",
    ">$$\\frac{\\partial \\text{E}}{\\partial \\text{net}_k} = \\frac{\\partial \\text{E}}{\\partial \\text{net}_t} \\frac{\\partial \\text{net}_t}{\\partial \\text{net}_{t-1}} \\frac{\\partial \\text{net}_{t-1}}{\\partial \\text{net}_{t-2}} \\dots \\frac{\\partial \\text{net}_{k+1}}{\\partial \\text{net}_{k}}\n",
    "$$\n",
    ">\n",
    ">$$\n",
    "= \\frac{\\partial \\text{E}}{\\partial \\text{net}_t} \\text{W} \\text{diag}(f^\\prime(net_{t-1})) \\times \\text{W} \\text{diag}(f^\\prime(net_{t-2})) \\dots \\text{W} \\text{diag}(f^\\prime(net_{k})) = \\frac{\\partial \\text{E}}{\\partial \\text{net}_t} \\prod_{i=k}^{t-1} \\text{W} \\text{diag}(f^\\prime(net_{i}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **然后，更新W：**循环层第l层的输入 $\\text{net}^l$ 与循环层第l-1层的输出$\\text{net}^{l-1}$关系如下：\n",
    ">\n",
    ">$$\n",
    "\\text{net}^l_t = \\text{W} s_{t-1} + \\text{U} a^{l-1}_t, a^{l-1}_t = f^{l-1}(\\text{net}^{l-1}_t)\n",
    "$$\n",
    ">\n",
    ">$$\n",
    "\\frac{\\partial \\text{net}^l_t}{\\partial \\text{net}^{l-1}_t} = \\frac{\\partial \\text{net}^l_t}{\\partial a^{l-1}_t} \\frac{\\partial a^{l-1}_t}{\\text{net}^{l-1}_t} = \\text{U} \\text{diag}(f^\\prime(\\text{net}^{l-1}_t))\n",
    "$$\n",
    "> 所以，\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\text{net}^{l-1}_t} = \\frac{\\partial E}{\\partial \\text{net}^{l}_t} \\text{U} \\text{diag}(f^\\prime(\\text{net}^{l-1}_t))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN的变形：\n",
    "\n",
    "* LSTM\n",
    "* GRU\n",
    "* 双向RNN\n",
    "* 深度RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 双向循环神经网络：\n",
    "\n",
    "<img src=\"NLP_github/bi_RNN.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "* 输出：$o_t = g(Vs_t + V^\\prime s_t^\\prime)$\n",
    "* 正向隐含层输出：$s_t = \\sigma (U x_t + W s_{t-1})$\n",
    "* 反向隐含层输出：$s_t^\\prime = \\sigma (U^\\prime x_t + W^\\prime s^\\prime_{t+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 深度循环神经网络：\n",
    "\n",
    "多个隐含层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 长短期记忆网络 LSTM："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP_github/LSTM.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **遗忘门 forget gate：对之前的信息保留还是遗忘 $f_t = \\sigma (W_f[h_{t-1}, x_t])$。**\n",
    "\n",
    "><img src=\"NLP_github/LSTM_1.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "2. **输入门 input gate以及新的候选值：哪部分信息将会被更新，以及新的信息。$i_t = \\sigma (W_i[h_{t-1}, x_t])， \\tilde C_t = tanh(W_c[h_{t-1}, x_t])$。**\n",
    "\n",
    "><img src=\"NLP_github/LSTM_2.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "3. **更新信息：$C_t = f_t * C_{t-1}+i_t*\\tilde C_t$。**\n",
    "\n",
    "><img src=\"NLP_github/LSTM_3.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "4. **当前层输出：$o_t = \\sigma (W_o[h_{t-1}, x_t]), h_t = o_t * tanh(C_t)$。**\n",
    "\n",
    "><img src=\"NLP_github/LSTM_4.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 GRU (Gate Recurrent Unit)："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU模型中只有两个门：分别是更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集ℎ̃𝑡上，重置门越小，前一状态的信息被写入的越少。\n",
    "\n",
    "<img src=\"NLP_github/GRU.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "* **更新门 (combines input and forget gates): $z_t = \\sigma (\\text{W}_z[h_{t-1}, x_t])$**\n",
    "\n",
    "* **重置门 $r_t = \\sigma (W_r[h_{t-1}, x_t])$**\n",
    "\n",
    "* **当前时刻的信息：$\\tilde h_t =tanh(W[r_t * h_{t−1},x_t])$**\n",
    "\n",
    "* **当前层输出：$h_t = (1 − z_t) * h_{t−1} + z_t * h_t$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RNN 几种不同架构：\n",
    "\n",
    "<img src=\"NLP_github/RNN_variant.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "* one to many: 一个输入对应多个输出，即这个架构多用于图片的对象识别，即输入一个图片，输出一个文本序列。\n",
    "* many to one: 多个输入对应一个输出，多用于文本分类或视频分类，即输入一段文本或视频片段，输出类别。\n",
    "* many to many: 1. 这种结构广泛的用于机器翻译，输入一个文本，输出另一种语言的文本。2. 序列标注。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN + Attention\n",
    "\n",
    "<img src=\"NLP_github/attention.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN不同架构: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "BPTT: https://zybuluo.com/hanbingtao/note/541458 \n",
    "\n",
    "RNN介绍: https://zhuanlan.zhihu.com/p/36455374\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
