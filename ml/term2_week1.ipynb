{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# run Spark locally with K worker threads (ideally set to number of cores)\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"COM6012 Spark Intro\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sharc-node152.shef.ac.uk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COM6012 Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=COM6012 Spark Intro>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 结构光谱：\n",
    "\n",
    "<img src=\"image_github/data_spectrum.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "* 结构性数据：数据库\n",
    "\n",
    "* 半结构数据：XML，JSON\n",
    "\n",
    "* 无结构数据：语言等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark：\n",
    "\n",
    "### 2.1 Spark 与 Hadoop 区别和联系：\n",
    "\n",
    "首先，两者均为大数据框架，但是解决问题的层面不同。Hadoop更类似于分布式基础设施，而spark更像是一种处理大数据的软件（类似于计算机与操作系统的关系）。因此，可以说spark基于Hadoop。当然，spark也可以运行在除此之外的数据系统平台。其次，spark处理数据速度更快。Spark多个任务之间数据通信是基于内存，而Hadoop是基于磁盘。\n",
    "\n",
    "### 2.2 RDD 与 Dataframe：\n",
    "\n",
    "如图所示，左侧为RDD结构右侧为Dataframe结构。相比于RDD，Dataframe为高层次的抽象，带有schema，并且dataframe带有更丰富的sql函数可以简化操作。但是，有些时候也需要将Dataframe转换为RDD。\n",
    "\n",
    "<img src=\"image_github/RDD_DF.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "### 2.3 Spark Components：\n",
    "\n",
    "spark项目首先创建SparkContext，通过其与Spark集群相连，可以理解为一个SparkContext就是一个spark application的生命周期，SparkContext连接Cluster manager，其分配资源给Spark executor，从而进行计算，获取数据等操作。\n",
    "<img src=\"image_github/manager.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "### 2.4 Spark 集群角色介绍：\n",
    "\n",
    "**Master和Worker是Spark的守护进程（系统进程），即Spark在特定模式下正常运行所必须的进程。Driver和Executor是临时进程（用户进程），当有具体任务提交到Spark集群才会开启的进程。**\n",
    "\n",
    "* Master：Spark特有资源调度系统的Leader。掌管着整个集群的资源信息。Master用来监听并管理所有的Worker，看Worker是否正常工作。\n",
    "\n",
    "\n",
    "* Worker：Spark特有资源调度系统的Slave，有多个。每个Slave掌管着所在节点的资源信息。\n",
    "\n",
    "\n",
    "* Driver：Spark的驱动器是执行开发程序中的main方法的进程。它负责开发人员编写的用来创建SparkContext、创建RDD，以及进行RDD的转化操作和行动操作代码的执行。（用户进程）\n",
    "\n",
    "\n",
    "* Executor：Spark Executor是一个工作进程，负责在 Spark 作业中运行任务，任务间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。\n",
    "\n",
    "<img src=\"image_github/cluster_example.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "### 2.5 Spark三种分布式部署模式：\n",
    "\n",
    "* Standalone：独立模式，Spark 原生的简单集群管理器， 自带完整的服务， 可单独部署到一个集群中，无需依赖任何其他资源管理系统， 使用 Standalone 可以很方便地搭建一个集群，一般在公司内部没有搭建其他资源管理框架的时候才会使用。\n",
    "<img src=\"image_github/standalone.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "* YARN：统一的资源管理机制， 在上面可以运行多套计算框架， 如map reduce、storm 等， 根据 driver 在集群中的位置不同，分为 yarn client 和 yarn cluster。**yarn client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。yarn cluster：Driver 程序运行在由ResourceManager 启动的 APPMaster 适用于生产环境。分担压力不会拖垮某个节点。**\n",
    "\n",
    "<img src=\"image_github/yarn.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "提交任务，App Submit； RM选择一个NM启动AM，AM来启动Driver（即初始化sc），yarn的cluster模式SparkAppMaster（用来申请资源，启动driver）和SparkContext在一个进程里边。AM（SparkAppMaster）向RM申请启动Executor；（默认情况下一个节点启一个executor这样子负载比较均衡，也可以启两个）。\n",
    "\n",
    "* Mesos："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDD：\n",
    "\n",
    "### 3.1 Resilient Distributed Datasets 弹性分布式数据集：\n",
    "\n",
    "RDD 是 Spark 提供的最重要的抽象概念，它是一种有容错机制的特殊数据集合，可以分布在集群的结点上，以函数式操作集合的方式进行各种并行操作。\n",
    "<img src=\"image_github/RDD.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "RDD 特性：\n",
    "\n",
    "1. 基于内存 In-Memory：可以全部或部分缓存在内存中，在多次计算间重用。\n",
    "\n",
    "2. 只读 Immutable：不能修改，只能通过转换操作生成新的 RDD。\n",
    "\n",
    "3. 分区 Partition：一个RDD分为多个分区，每个分区可存储在相同或不同的节点中，如上图所示。**（分布式）**\n",
    "\n",
    "4. 容错 Fault-tolerant：RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。**（弹性）**\n",
    "\n",
    "### 3.2 RDD 操作：\n",
    "\n",
    "**transformation（lazy evaluation）and action：创建新的数据集但是不会进行计算，记住中间转换的步骤，当遇到action时，才会真正计算。**\n",
    "\n",
    "<img src=\"image_github/transformation.png\" width=\"500\" height=\"500\">\n",
    "<img src=\"image_github/action.png\" width=\"500\" height=\"500\">\n",
    "<img src=\"image_github/RDD_step.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "**persistence/cache：将数据存储到内存中。**\n",
    "\n",
    "**key-values RDD：**\n",
    "<img src=\"image_github/RDD_key_value.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "### 3.3 共享变量：\n",
    "\n",
    "* 广播变量 Broadcast variables\n",
    "* 累加器 Accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataframe：\n",
    "\n",
    "<img src=\"image_github/speed.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Python：\n",
    "\n",
    "### 5.1 创建 RDD 与 Dataframe：\n",
    "\n",
    "#### 创建 RDD：\n",
    "\n",
    "1. 并行集合(Parallelized Collections): 从内存里直接读取数据。\n",
    "2. 文件系统数据集: Hadoop Datasets 或文本文件,比如通过SparkContext.textFile()读取的数据。\n",
    "\n",
    "#### 创建 Dataframe：spark.read.text()  与 spark.read.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Aug95 数据格式：\n",
    "\n",
    "<img src=\"image_github/NASA-format.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "Advertising 数据格式：\n",
    "\n",
    "<img src=\"image_github/ad_format.png\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# create RDD.\n",
    "nums = sc.parallelize([(1,2,3),(4,5,6),(7,8,9)])\n",
    "print(type(nums))\n",
    "# create RDD.\n",
    "logFile_rdd=sc.textFile(\"../Data/NASA_Aug95_100.txt\")\n",
    "print(type(logFile_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame.\n",
    "logFile_df=spark.read.text(\"../Data/NASA_Aug95_100.txt\") # from .txt\n",
    "advertise_df = spark.read.load(\"../Data/Advertising.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\") # from .csv\n",
    "print(type(logFile_df))\n",
    "print(type(advertise_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 展示 RDD 与 Dataframe：\n",
    "\n",
    "#### RDD中collect函数：\n",
    "\n",
    "**此方法可以将RDD类型的数据转化为数组**，同时会从远程集群是拉取数据到driver端。若放在分布式环境下运行，一次collect操作会将分布式各个节点上的数据汇聚到一个driver节点上，而这么一来，后续所执行的运算和操作就会脱离这个分布式环境而相当于单机环境下运行。并且，**将大量数据汇集到一个driver节点上，将数据用数组存放，占用了jvm堆内存，会导致内存溢出。可以使用take(n)。**\n",
    "\n",
    "#### Dataframe中show()与describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839',\n",
       " 'uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0',\n",
       " 'uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|in24.inetnebr.com...|\n",
      "|uplherc.upl.com -...|\n",
      "|uplherc.upl.com -...|\n",
      "|uplherc.upl.com -...|\n",
      "|uplherc.upl.com -...|\n",
      "|ix-esc-ca2-07.ix....|\n",
      "|uplherc.upl.com -...|\n",
      "|slppp6.intermind....|\n",
      "|piweba4y.prodigy....|\n",
      "|slppp6.intermind....|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logFile_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               _c0|               TV|             radio|         newspaper|             sales|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|               200|              200|               200|               200|               200|\n",
      "|   mean|             100.5|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|57.879184513951124|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|                 1|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|               200|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "advertise_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 RDD 与 Dataframe 相互转换：.toDF() 与 .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DF.\n",
    "df1 = nums.toDF([\"a\", \"b\", \"c\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DF to RDD.\n",
    "rdd1 = logFile_df.rdd\n",
    "type(rdd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 练习："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Jupyther notebook is primarily for interactive learning that is more suitable on small scale data. For large-scale (big) data, stand-alone programs or HPC batch jobs are encouraged. \n",
    "* Load the NASA access log Aug95 data in Session 1 and create a DataFrame with FIVE columns by **specifying** the schema \n",
    "* Perform some more challenging mining tasks in Session 1 using *as many DataFrame functions as possible*\n",
    "   * How many **unique** hosts on a particular day (e.g., 15th August)?\n",
    "   * How many **unique** hosts in total (i.e., in August 1995)?\n",
    "   * Which host is the most frequent visitor?\n",
    "   * How many different types of return codes?\n",
    "   * How many requests per day on average?\n",
    "   * How many requests per post on average?\n",
    "   * Any other question that you are interested in.\n",
    "* Explore more CSV data of your interest via Google or at [Sample CSV data](https://support.spatialkey.com/spatialkey-sample-csv-data/), including insurance, real estate, and sales transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logFile=spark.read.text(\"../Data/NASA_access_log_Aug95.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839|\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0                                                   |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0                          |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0                        |\n",
      "|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0                           |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logFile.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正则表达式-分组或特征标群：**\n",
    "<img src=\"image_github/regex.png\" width=\"600\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_logs = [item['value'] for item in logFile.take(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GET', '/shuttle/missions/sts-68/news/sts-68-mcc-05.txt', 'HTTP/1.0'),\n",
       " ('GET', '/', 'HTTP/1.0'),\n",
       " ('GET', '/images/ksclogo-medium.gif', 'HTTP/1.0'),\n",
       " ('GET', '/images/MOSAIC-logosmall.gif', 'HTTP/1.0'),\n",
       " ('GET', '/images/USA-logosmall.gif', 'HTTP/1.0'),\n",
       " ('GET', '/images/launch-logo.gif', 'HTTP/1.0'),\n",
       " ('GET', '/images/WORLD-logosmall.gif', 'HTTP/1.0'),\n",
       " ('GET', '/history/skylab/skylab.html', 'HTTP/1.0'),\n",
       " ('GET', '/images/launchmedium.gif', 'HTTP/1.0'),\n",
       " ('GET', '/history/skylab/skylab-small.gif', 'HTTP/1.0'),\n",
       " ('GET', '/images/ksclogosmall.gif', 'HTTP/1.0'),\n",
       " ('GET', '/history/apollo/images/apollo-logo1.gif', 'HTTP/1.0'),\n",
       " ('GET', '/history/apollo/images/apollo-logo.gif', 'HTTP/1.0'),\n",
       " ('GET', '/images/NASA-logosmall.gif', 'HTTP/1.0'),\n",
       " ('GET', '/shuttle/missions/sts-69/mission-sts-69.html', 'HTTP/1.0')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "method_uri_protocol = [re.search(method_uri_protocol_pattern, item).groups() for item in sample_logs]\n",
    "method_uri_protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s' # \"\\\" escape character. \"S\" match everyting rather than space.\n",
    "ts_date_pattern = r'(\\d{2}/\\w{3}/\\d{4})' # \\d number.\\w word. {} times.\n",
    "ts_time_pattern = r'(:\\d{2}:\\d{2}:\\d{2})'\n",
    "method_url_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "content_size_pattern = r'\\s(\\d+)$'\n",
    "\n",
    "df = logFile.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "               regexp_extract('value', ts_date_pattern, 1).alias('date'),  \n",
    "               regexp_extract('value', ts_time_pattern, 1).alias('time_str'),  \n",
    "               regexp_extract('value', method_url_protocol_pattern, 1).alias('method'),\n",
    "               regexp_extract('value', method_url_protocol_pattern, 2).alias('address'),\n",
    "               regexp_extract('value', method_url_protocol_pattern, 3).alias('protocol'),\n",
    "               regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "               regexp_extract('value', content_size_pattern, 1).cast('integer').alias('size')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+------+--------------------+--------+------+-----+\n",
      "|                host|       date| time_str|method|             address|protocol|status| size|\n",
      "+--------------------+-----------+---------+------+--------------------+--------+------+-----+\n",
      "|   in24.inetnebr.com|01/Aug/1995|:00:00:01|   GET|/shuttle/missions...|HTTP/1.0|   200| 1839|\n",
      "|     uplherc.upl.com|01/Aug/1995|:00:00:07|   GET|                   /|HTTP/1.0|   304|    0|\n",
      "|     uplherc.upl.com|01/Aug/1995|:00:00:08|   GET|/images/ksclogo-m...|HTTP/1.0|   304|    0|\n",
      "|     uplherc.upl.com|01/Aug/1995|:00:00:08|   GET|/images/MOSAIC-lo...|HTTP/1.0|   304|    0|\n",
      "|     uplherc.upl.com|01/Aug/1995|:00:00:08|   GET|/images/USA-logos...|HTTP/1.0|   304|    0|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995|:00:00:09|   GET|/images/launch-lo...|HTTP/1.0|   200| 1713|\n",
      "|     uplherc.upl.com|01/Aug/1995|:00:00:10|   GET|/images/WORLD-log...|HTTP/1.0|   304|    0|\n",
      "|slppp6.intermind.net|01/Aug/1995|:00:00:10|   GET|/history/skylab/s...|HTTP/1.0|   200| 1687|\n",
      "|piweba4y.prodigy.com|01/Aug/1995|:00:00:10|   GET|/images/launchmed...|HTTP/1.0|   200|11853|\n",
      "|slppp6.intermind.net|01/Aug/1995|:00:00:11|   GET|/history/skylab/s...|HTTP/1.0|   200| 9202|\n",
      "|slppp6.intermind.net|01/Aug/1995|:00:00:12|   GET|/images/ksclogosm...|HTTP/1.0|   200| 3635|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995|:00:00:12|   GET|/history/apollo/i...|HTTP/1.0|   200| 1173|\n",
      "|slppp6.intermind.net|01/Aug/1995|:00:00:13|   GET|/history/apollo/i...|HTTP/1.0|   200| 3047|\n",
      "|     uplherc.upl.com|01/Aug/1995|:00:00:14|   GET|/images/NASA-logo...|HTTP/1.0|   304|    0|\n",
      "|        133.43.96.45|01/Aug/1995|:00:00:16|   GET|/shuttle/missions...|HTTP/1.0|   200|10566|\n",
      "|kgtyk4.kj.yamagat...|01/Aug/1995|:00:00:17|   GET|                   /|HTTP/1.0|   200| 7280|\n",
      "|kgtyk4.kj.yamagat...|01/Aug/1995|:00:00:18|   GET|/images/ksclogo-m...|HTTP/1.0|   200| 5866|\n",
      "|     d0ucr6.fnal.gov|01/Aug/1995|:00:00:19|   GET|/history/apollo/a...|HTTP/1.0|   200| 2743|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995|:00:00:19|   GET|/shuttle/resource...|HTTP/1.0|   200| 6849|\n",
      "|     d0ucr6.fnal.gov|01/Aug/1995|:00:00:20|   GET|/history/apollo/a...|HTTP/1.0|   200|14897|\n",
      "+--------------------+-----------+---------+------+--------------------+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去重：df.select(df.index).distinct().count() 或 df.groupBy('index').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4211"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.filter(df['date'] == '15/Aug/1995')\n",
    "\n",
    "df2 = df1.groupBy('host').count() \n",
    "\n",
    "df2.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75029"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df.groupBy('host').count()\n",
    "\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                host|count|\n",
      "+--------------------+-----+\n",
      "|  edams.ksc.nasa.gov| 6530|\n",
      "|piweba4y.prodigy.com| 4846|\n",
      "|        163.206.89.4| 4791|\n",
      "|piweba5y.prodigy.com| 4607|\n",
      "|piweba3y.prodigy.com| 4416|\n",
      "|www-d1.proxy.aol.com| 3889|\n",
      "|www-b2.proxy.aol.com| 3534|\n",
      "|www-b3.proxy.aol.com| 3463|\n",
      "|www-c5.proxy.aol.com| 3423|\n",
      "|www-b5.proxy.aol.com| 3411|\n",
      "|www-c2.proxy.aol.com| 3407|\n",
      "|www-d2.proxy.aol.com| 3404|\n",
      "|www-a2.proxy.aol.com| 3337|\n",
      "|         news.ti.com| 3298|\n",
      "|www-d3.proxy.aol.com| 3296|\n",
      "|www-b4.proxy.aol.com| 3293|\n",
      "|www-c3.proxy.aol.com| 3272|\n",
      "|www-d4.proxy.aol.com| 3234|\n",
      "|www-c1.proxy.aol.com| 3177|\n",
      "|www-c4.proxy.aol.com| 3134|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|status|  count|\n",
      "+------+-------+\n",
      "|   501|     27|\n",
      "|   500|      3|\n",
      "|   400|     10|\n",
      "|   403|    171|\n",
      "|   404|  10056|\n",
      "|   200|1398988|\n",
      "|   304| 134146|\n",
      "|   302|  26497|\n",
      "+------+-------+\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "code = df.groupBy('status').count()\n",
    "code.show()\n",
    "\n",
    "print(code.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = df.select('host', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                host|       date|\n",
      "+--------------------+-----------+\n",
      "|   in24.inetnebr.com|01/Aug/1995|\n",
      "|     uplherc.upl.com|01/Aug/1995|\n",
      "|     uplherc.upl.com|01/Aug/1995|\n",
      "|     uplherc.upl.com|01/Aug/1995|\n",
      "|     uplherc.upl.com|01/Aug/1995|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995|\n",
      "|     uplherc.upl.com|01/Aug/1995|\n",
      "|slppp6.intermind.net|01/Aug/1995|\n",
      "|piweba4y.prodigy.com|01/Aug/1995|\n",
      "|slppp6.intermind.net|01/Aug/1995|\n",
      "|slppp6.intermind.net|01/Aug/1995|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995|\n",
      "|slppp6.intermind.net|01/Aug/1995|\n",
      "|     uplherc.upl.com|01/Aug/1995|\n",
      "|        133.43.96.45|01/Aug/1995|\n",
      "|kgtyk4.kj.yamagat...|01/Aug/1995|\n",
      "|kgtyk4.kj.yamagat...|01/Aug/1995|\n",
      "|     d0ucr6.fnal.gov|01/Aug/1995|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995|\n",
      "|     d0ucr6.fnal.gov|01/Aug/1995|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|day|count|\n",
      "+---+-----+\n",
      "| 07|57362|\n",
      "| 15|58847|\n",
      "| 11|61246|\n",
      "| 29|67988|\n",
      "| 30|80641|\n",
      "| 01|33996|\n",
      "| 22|57762|\n",
      "| 28|55496|\n",
      "| 16|56653|\n",
      "| 31|90125|\n",
      "| 18|56246|\n",
      "| 27|32823|\n",
      "| 17|58988|\n",
      "| 26|31608|\n",
      "| 09|60458|\n",
      "| 05|31893|\n",
      "| 19|32094|\n",
      "| 23|58097|\n",
      "| 08|60157|\n",
      "| 03|41388|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "day = r'^(\\d{2})'\n",
    "df5 = df4.withColumn('day', regexp_extract(df4['date'], day, 1))\n",
    "\n",
    "df6 = df5.groupBy('day').count()\n",
    "\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(count)|\n",
      "+------------------+\n",
      "|52329.933333333334|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(mean('count')).show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-spark]",
   "language": "python",
   "name": "conda-env-jupyter-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
