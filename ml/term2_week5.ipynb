{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 PCA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基于RDD的数据类型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Local vector：dense and sparse vector\n",
    "\n",
    "A local vector has integer-typed and **0-based indices** and double-typed values, **stored on a single machine**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse vector:  (3,[0,2],[1.0,3.0])\n",
      "dense vector:  [1.0,2.0,3.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors # from pyspark.ml.linalg import Vectors\n",
    "\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "\n",
    "dv1 = Vectors.dense([1, 2, 3])\n",
    "\n",
    "print(\"sparse vector: \", sv1)\n",
    "print(\"dense vector: \", dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 labeled point："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laebl:  0.0\n",
      "features:  (3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))\n",
    "\n",
    "print(\"laebl: \", neg.label)\n",
    "print(\"features: \", neg.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Local matrix：dense and sparse\n",
    "\n",
    "**Matrices.sparse(row, column, [column index], [row index], [values]) 稀疏矩阵的列索引采取CSC格式。首先，在第一列前添加全为零的列，因此，第一列的非零值的个数为零，之后，每一列的值为当前列非零值的个数加前一列非零值的个数。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 2.],\n",
      "             [3., 4.],\n",
      "             [5., 6.]])\n",
      "3 X 2 CSCMatrix\n",
      "(0,0) 9.0\n",
      "(2,1) 6.0\n",
      "(1,1) 8.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# dense matrix.\n",
    "dm = Matrices.dense(3, 2, [1, 3, 5, 2, 4, 6])\n",
    "\n",
    "# sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8]) # 0, 0+1, 0+1+2  => 0, 1, 3\n",
    "\n",
    "print(dm)\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 distributed matrix：\n",
    "\n",
    "* RowMatrix\n",
    "* IndexedRowMatrix\n",
    "* CoordinateMatrix\n",
    "* BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x2ae2f75bc9e8>\n",
      "[DenseVector([1.0, 2.0, 3.0]), DenseVector([4.0, 5.0, 6.0]), DenseVector([7.0, 8.0, 9.0]), DenseVector([10.0, 11.0, 12.0])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "rowsRDD = mat.rows\n",
    "\n",
    "print(rowsRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA：see week7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataframe："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "| (5,[1,3],[1.0,7.0])|\n",
      "|[2.0,0.0,3.0,4.0,...|\n",
      "|[4.0,0.0,0.0,6.0,...|\n",
      "+--------------------+\n",
      "\n",
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.7944, 0.2056, 0.0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainedVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RDD："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征分解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[-0.44859172, -0.28423808,  0.08344545],\n",
      "             [ 0.13301986, -0.05621156,  0.04423979],\n",
      "             [-0.12523156,  0.76362648, -0.57807123],\n",
      "             [ 0.21650757, -0.56529588, -0.79554051],\n",
      "             [-0.84765129, -0.11560341, -0.15501179]])\n",
      "[DenseVector([1.6486, -4.0133, -5.5245]), DenseVector([-4.6451, -1.1168, -5.5245]), DenseVector([-6.4289, -5.338, -5.5245])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "# Principal components (axis 3 dimensional) \n",
    "pc = mat.computePrincipalComponents(3)\n",
    "print(pc)\n",
    "# dataset after dimensional reduction \n",
    "projected = mat.multiply(pc)\n",
    "print(projected.rows.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 特征向量矩阵 pc = mat.computePrincipalComponents(3) \n",
    "\n",
    "\n",
    "* 降维后的数据 = 原数据乘特征向量 projected = mat.multiply(pc) \n",
    "\n",
    "**注意：上述方法会对数据自动进行中心化处理。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 奇异值分解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.029275535600473,5.368578733451684,2.5330498218813755]\n",
      "DenseMatrix([[-0.31278534,  0.31167136,  0.30366911],\n",
      "             [-0.02980145, -0.17133211, -0.02226069],\n",
      "             [-0.12207248,  0.15256471, -0.95070998],\n",
      "             [-0.71847899, -0.68096285, -0.0172245 ],\n",
      "             [-0.60841059,  0.62170723,  0.05606596]])\n"
     ]
    }
   ],
   "source": [
    "# Compute the top 5 singular values and corresponding singular vectors.\n",
    "svd = mat.computeSVD(3, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix. 特征向量\n",
    "\n",
    "print(s)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **svd = mat.computeSVD(3, computeU=True) 需要对数据进行中心化处理。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[-0.44859172, -0.28423808, -0.81664677],\n",
      "             [ 0.13301986, -0.05621156, -0.03622012],\n",
      "             [-0.12523156,  0.76362648, -0.34267361],\n",
      "             [ 0.21650757, -0.56529588, -0.13898906],\n",
      "             [-0.84765129, -0.11560341,  0.44162541]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "#We center the data to remove the mean. \n",
    "standardizer = StandardScaler(True, False)\n",
    "model = standardizer.fit(rows)\n",
    "centeredRows = model.transform(rows)\n",
    "centeredRows.collect()\n",
    "centeredmat = RowMatrix(centeredRows)\n",
    "\n",
    "# Compute the top 3 singular values and corresponding singular vectors.\n",
    "svd = centeredmat.computeSVD(3, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-spark]",
   "language": "python",
   "name": "conda-env-jupyter-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
