{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树2："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 分类树与回归树：\n",
    "\n",
    "<img src=\"image_github/regression_tree.png\" width=\"600\" height=\"500\">\n",
    "\n",
    "决策树不仅可以用来解决分类问题，还可以处理回归问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 决策树的4种度量：\n",
    "\n",
    "> 1. 信息增益 \n",
    "2. 方差 \n",
    "3. 基尼指数 \n",
    "4. 信息增益率 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 信息增益 IG - 决策树：\n",
    "\n",
    ">$$\n",
    "\\text{IG}(D, A) = \\text{H}(D) - \\text{H}(D|A)\n",
    "$$\n",
    "\n",
    ">**对一个确定的数据集来说，H(D)是确定的，那H(D|A)在A特征一定的情况下，随机变量的不确定性越小，信息增益越大，这个特征的表现就越好。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 方差：\n",
    "\n",
    ">与ID3使用熵构建决策树类似，**通过每次选择最小方差（impurity，variance）构建回归树。**\n",
    "\n",
    "><img src=\"image_github/CART.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "例如：\n",
    "\n",
    "<img src=\"image_github/CART_2.png\" width=\"400\" height=\"500\">\n",
    "<img src=\"image_github/CART_1.png\" width=\"550\" height=\"500\">\n",
    "<img src=\"image_github/CART_3.png\" width=\"550\" height=\"500\">\n",
    "<img src=\"image_github/CART_4.png\" width=\"550\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 基尼指数 Gini index - CART 分类回归树:\n",
    "\n",
    ">**表示一个随机选中的样本在子集中被分错的可能性。注意：基尼指数不等于基尼系数。**\n",
    "<img src=\"image_github/gini_index.png\" width=\"300\" height=\"400\">\n",
    "CART 是一棵严格二叉树。每次分裂只做二分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 信息增益率 information gain ratio:\n",
    "\n",
    "><img src=\"image_github/IGR.png\" width=\"400\" height=\"400\">\n",
    "\n",
    ">**信息增益偏向于选择取值较多的特征（如：winter, summer ...），但根据熵的公式可知，特征越多，熵越大，所以除A特征的熵正好抵消了特征变量的复杂程度。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 过拟合：\n",
    "\n",
    "在小数据集情况下，树的深度越深越容易造成过拟合。\n",
    "\n",
    "### 3.1 先剪枝 Pre-pruning:\n",
    "\n",
    "通过提前停止树的构建而对树“剪枝”，一旦停止，节点就成为树叶。该树叶可以持有子集元组中最频繁的类。\n",
    "\n",
    "### 3.2 后剪枝 Post-pruning:\n",
    "\n",
    "它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。\n",
    "\n",
    "<img src=\"image_github/post_pruning.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型集成:\n",
    "\n",
    "https://blog.csdn.net/sinat_26917383/article/details/54667077\n",
    "\n",
    "<img src=\"image_github/model_ensemble.png\" width=\"700\" height=\"500\">\n",
    "\n",
    "### 4.1 Boosting:\n",
    "\n",
    "Boosting的思想是一种迭代的方法，每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例。最终将这些弱分类器进行加权相加。（对数据集进行权重调整）\n",
    "\n",
    "<img src=\"image_github/boosting_1.png\" width=\"500\" height=\"500\">\n",
    "<img src=\"image_github/boosting_2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    "### 4.2 Bagging:\n",
    "\n",
    "Bagging就是采用有放回的方式进行抽样，用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合。（例如：随机森林）\n",
    "\n",
    "* bootstrap samples(random sample): 数据集随机选择的子集。\n",
    "\n",
    "<img src=\"image_github/bagging.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：\n",
    "\n",
    "<img src=\"image_github/boost_bag.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. pySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 DT\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fixed acidity: string, volatile acidity: string, citric acid: string, residual sugar: string, chlorides: string, free sulfur dioxide: string, total sulfur dioxide: string, density: string, pH: string, sulphates: string, alcohol: string, quality: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = spark.read.csv('../Data/winequality-white.csv', sep=';', header='true')\n",
    "rawdata.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: string (nullable = true)\n",
      " |-- volatile acidity: string (nullable = true)\n",
      " |-- citric acid: string (nullable = true)\n",
      " |-- residual sugar: string (nullable = true)\n",
      " |-- chlorides: string (nullable = true)\n",
      " |-- free sulfur dioxide: string (nullable = true)\n",
      " |-- total sulfur dioxide: string (nullable = true)\n",
      " |-- density: string (nullable = true)\n",
      " |-- pH: string (nullable = true)\n",
      " |-- sulphates: string (nullable = true)\n",
      " |-- alcohol: string (nullable = true)\n",
      " |-- quality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据类型转换：string to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# string to double using cast.\n",
    "schemaNames = rawdata.schema.names\n",
    "ncolumns = len(rawdata.columns)\n",
    "from pyspark.sql.types import DoubleType\n",
    "for i in range(ncolumns):\n",
    "    rawdata = rawdata.withColumn(schemaNames[i], rawdata[schemaNames[i]].cast(DoubleType()))\n",
    "rawdata = rawdata.withColumnRenamed('quality', 'labels')\n",
    "rawdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据转换成 Vector:\n",
    "\n",
    "* **RDD: map(lambda r: [Vectors.dense(r[:-1]), r[-1]]).toDF(['features','label'])**\n",
    "* **Dataframe: vectorAssembler(A feature transformer that merges multiple columns into a vector column).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = schemaNames[0:ncolumns-1], outputCol = 'features') \n",
    "raw_plus_vector = assembler.transform(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|labels|\n",
      "+--------------------+------+\n",
      "|[7.0,0.27,0.36,20...|   6.0|\n",
      "|[6.3,0.3,0.34,1.6...|   6.0|\n",
      "|[8.1,0.28,0.4,6.9...|   6.0|\n",
      "|[7.2,0.23,0.32,8....|   6.0|\n",
      "|[7.2,0.23,0.32,8....|   6.0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = raw_plus_vector.select('features','labels')\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树 - 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.762375 \n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], 50)\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"labels\", featuresCol=\"features\", maxDepth=5)\n",
    "model = dt.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator\\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %g \" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成学习 - bagging：随机森林 random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.725299 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(labelCol=\"labels\", featuresCol=\"features\", maxDepth=5, numTrees=3)\n",
    "model = rfr.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator\\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %g \" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成学习 - boosting：梯度提升树  Gradient Boosting or Gradient-boosted trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.747853 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbtr = GBTRegressor(labelCol=\"labels\", featuresCol=\"features\", maxDepth=5, maxIter=5, lossType='squared')\n",
    "model = gbtr.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator \\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %g \" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-spark]",
   "language": "python",
   "name": "conda-env-jupyter-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
