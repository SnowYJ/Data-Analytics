{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 协同过滤："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于用户的协同过滤算法是通过用户的历史行为数据发现用户对商品或内容的喜欢(如商品购买，收藏，内容评论或分享)，并对这些喜好进行度量和打分。根据不同用户对相同商品或内容的态度和偏好程度**计算用户之间的关系**。**在有相同喜好的用户间进行商品推荐**。简单的说就是如果A,B两个用户都购买了x,y,z三本图书，并且给出了5星的好评。那么A和B就属于同一类用户。可以将A看过的图书w也推荐给用户B。\n",
    "\n",
    "https://www.jianshu.com/p/d15ba37755d1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image_github/collaborative_filter.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**挑战：新用户，新商品，新社区。以及，大规模的数据，矩阵的稀疏。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 协同过滤方法：\n",
    "\n",
    "1. **Memory-based:** \n",
    "\n",
    "    **基于项目的(Item一based)协同过滤是根据用户对相似项目的评分数据预测目标项目的评分。**\n",
    "\n",
    "    **基于用户的(User-based)协同过滤是通过相似用户对项目的评分推荐给目标用户。**\n",
    "\n",
    "\n",
    "2. **Model-based:**\n",
    "    \n",
    "    **使用机器学习算法进行预测。例如：矩阵分解**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 矩阵分解 Matrix Factorisation - 隐含语义模型：\n",
    "\n",
    "矩阵分解可以用于实现隐含语义模型。例如在电影推荐系统中，将用户-电影-等级矩阵分解成用户矩阵以及电影矩阵，其中，每个用户以及电影由一个向量表示 类似于word-embedding（下图所示）。\n",
    "\n",
    "<img src=\"image_github/m_f.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "$p_u$: 用户u。\n",
    "\n",
    "$q_i$: 物品i。\n",
    "\n",
    "$r_{ui} = q_i^T p_u$: 用户u对物品i的评价等级。\n",
    "\n",
    "**注意：由于rating矩阵中存在缺失值，因此，不能使用奇异值分解方法进行矩阵分解。首先，需要对缺失值进行填充，然后在利用SVD进行矩阵分解。使用方法：交替最小二乘法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 交替最小二乘法 ALS：\n",
    "\n",
    "#### 1. 矩阵分解损失函数为：\n",
    "\n",
    "<img src=\"image_github/object_function.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "**注意：由于 $p_u$，$q_i$均未知，无法使用最小二乘法，梯度下降等算法。解决办法：交替最小二乘法。**\n",
    "\n",
    "#### 2. 交替最小二乘法：\n",
    "固定其中一项然后不断交替直到代价函数小于阈值，例如：首先固定物品向量p，根据最小二乘法求q。然后，根据得到的q求p，以此类推。重复上述步骤，直至收敛。由于上述步骤为交替对p以及q使用**最小二乘法**，因此最后会收敛。\n",
    "\n",
    "<img src=\"image_github/fix_p.png\" width=\"300\" height=\"300\">\n",
    "\n",
    " 或者使用**梯度下降**更新参数公式如下所示：\n",
    "\n",
    "<img src=\"image_github/m_f_equation.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluator: MAE 或 RMSE**\n",
    "\n",
    "<img src=\"image_github/evaluator.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. pyspark："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Collaborative Filtering RecSys\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用movielen数据，其格式如下所示：**\n",
    "<img src=\"image_github/movie_format.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "**pyspark.sql.Row：类似于字典的对象，可以通过Row(index = 'value')进行创建，可通过row.index对象形式或row['index']字典形式访问值。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='196\\t242\\t3\\t881250949'),\n",
       " Row(value='186\\t302\\t3\\t891717742'),\n",
       " Row(value='22\\t377\\t1\\t878887116'),\n",
       " Row(value='244\\t51\\t2\\t880606923'),\n",
       " Row(value='166\\t346\\t1\\t886397596')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.read.text(\"../Data/MovieLens100k.data\").rdd\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：spark.read.text()创建dataframe然后转换成rdd，其中，每一行为Row对象。直接使用sc.textFile()创建rdd，每一行为string类型。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+------+\n",
      "|movieId|rating|timestamp|userId|\n",
      "+-------+------+---------+------+\n",
      "|    242|   3.0|881250949|   196|\n",
      "|    302|   3.0|891717742|   186|\n",
      "|    377|   1.0|878887116|    22|\n",
      "|     51|   2.0|880606923|   244|\n",
      "|    346|   1.0|886397596|   166|\n",
      "|    474|   4.0|884182806|   298|\n",
      "|    265|   2.0|881171488|   115|\n",
      "|    465|   5.0|891628467|   253|\n",
      "|    451|   3.0|886324817|   305|\n",
      "|     86|   3.0|883603013|     6|\n",
      "+-------+------+---------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parts = lines.map(lambda row: row.value.split(\"\\t\"))\n",
    "\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "ratings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.select(ratings.userId).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.groupBy('userId').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.9, 0.1], 1234) # 1234 random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coldStartStrategy：测试集可能包含训练集未出现的用户或电影，'drop'参数会对这些数据进行忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7322716775792905\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=10, regParam=0.1, rank=3, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 给用户推荐电影："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[[1554, 6.7041345...|\n",
      "|   463|[[1175, 4.64767],...|\n",
      "|   833|[[1463, 5.290497]...|\n",
      "|   496|[[1463, 5.6878576...|\n",
      "|   148|[[1463, 5.7998896...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show(5)\n",
    "\n",
    "userRecs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对特定用户进行推荐电影："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|    26|\n",
      "|    29|\n",
      "|   474|\n",
      "|   191|\n",
      "|    65|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|    22|[[1463, 5.755835]...|\n",
      "|    20|[[1554, 5.4050055...|\n",
      "|    23|[[1463, 5.067517]...|\n",
      "|    25|[[814, 5.2719564]...|\n",
      "|    24|[[814, 5.761121],...|\n",
      "|    21|[[1463, 5.342782]...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = ratings.select('userId').distinct() # als.getUserCol()\n",
    "users.show(5)\n",
    "\n",
    "u1 = users.filter(users.userId.between(20, 25))\n",
    "userSubsetRecs = model.recommendForUserSubset(u1, 6)\n",
    "\n",
    "userSubsetRecs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据电影推荐用户："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|    463|[[4, 4.917675], [...|\n",
      "|   1591|[[519, 5.2030096]...|\n",
      "|    496|[[688, 6.1449723]...|\n",
      "|   1238|[[688, 4.5332084]...|\n",
      "|   1342|[[239, 4.05011], ...|\n",
      "|    833|[[507, 4.5306754]...|\n",
      "|    471|[[688, 4.9977026]...|\n",
      "|   1580|[[688, 2.0568476]...|\n",
      "|   1088|[[127, 5.0710692]...|\n",
      "|   1645|[[4, 6.089705], [...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = ratings.select('movieId').distinct() # 1682.\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)\n",
    "movieSubSetRecs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParamGridBuilder 与 CrossValidator："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9189286346635134\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "als = ALS(maxIter=10, rank=10, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(als.regParam, [0.01, 0.1, 0.3, 0.5]).build()\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "prediction = cvModel.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1693286927952753, 0.9479240219498275, 0.9856798579121662, 1.0723463487332474]\n",
      "ALS_4604aa70b1f0f7cfdbac\n",
      "({Param(parent='ALS_4604aa70b1f0f7cfdbac', name='regParam', doc='regularization parameter (>= 0).'): 0.01}, 1.1693286927952753)\n"
     ]
    }
   ],
   "source": [
    "params = cvModel.getEstimatorParamMaps()\n",
    "avgMetrics = cvModel.avgMetrics\n",
    "print(avgMetrics)\n",
    "print(cvModel.bestModel)\n",
    "\n",
    "all_params = list(zip(params, avgMetrics))\n",
    "best_param = sorted(all_params, key=lambda x: x[1], reverse=True)[0] # good => bad.\n",
    "\n",
    "print(best_param)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-spark]",
   "language": "python",
   "name": "conda-env-jupyter-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
