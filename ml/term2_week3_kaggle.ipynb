{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kaggle - Santander Product Recommendation\n",
    "\n",
    "**Data Description:**\n",
    "In this competition, you are provided with 1.5 years of customers behavior data from Santander bank to predict what new products customers will purchase. The data starts at 2015-01-28 and has monthly records of products a customer has, such as \"credit card\", \"savings account\", etc. You will predict what additional products a customer will get in the last month, 2016-06-28, in addition to what they already have at 2016-05-28. These products are the columns named: ind_(xyz)_ult1, which are the columns #25 - #48 in the training data. You will predict what a customer will buy in addition to what they already had at 2016-05-28. \n",
    "\n",
    "The test and train sets are split by time, and public and private leaderboard sets are split randomly.\n",
    "\n",
    "kaggle: https://www.kaggle.com/c/santander-product-recommendation/data\n",
    "\n",
    "help: https://www.elenacuoco.com/2016/12/22/alternating-least-squares-als-spark-ml/?cn-reloaded=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Collaborative Filtering RecSys\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 将用户id，日期以及24个金融产品作为列名（0买，1没买）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 24 financial product\n",
    "df = spark.read.csv(\"../Data/santander-product-recommendation/train_ver2.csv\", header=\"true\",inferSchema=\"true\")\\\n",
    ".select('ncodpers','fecha_dato',\n",
    "        'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', \n",
    "        'ind_cco_fin_ult1', 'ind_cder_fin_ult1', \n",
    "        'ind_cno_fin_ult1', 'ind_ctju_fin_ult1', \n",
    "        'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', \n",
    "        'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1', \n",
    "        'ind_deme_fin_ult1', 'ind_dela_fin_ult1', \n",
    "        'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', \n",
    "        'ind_hip_fin_ult1', 'ind_plan_fin_ult1', \n",
    "        'ind_pres_fin_ult1', 'ind_reca_fin_ult1', \n",
    "        'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', \n",
    "        'ind_viv_fin_ult1', 'ind_nomina_ult1', \n",
    "        'ind_nom_pens_ult1', 'ind_recibo_ult1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 获取日期为05-28以及06-28："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_05 = df.filter(df['fecha_dato']=='2015-05-28')\n",
    "df_06 = df.filter(df['fecha_dato']=='2015-06-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**测试集格式：**\n",
    "<img src=\"image_github/kaggle_bank_test_csv.png\" width=\"900\" height=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lista_users= spark.read.csv(\"../Data/santander-product-recommendation/test_ver2.csv\",header=\"true\",inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 对24个金融产品分配id： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# itemcol: product id. \n",
    "ratingsRDD_06 = df_06.rdd.map(lambda p: Row(userId=p[0], itemCol=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],rating=p[-24:]))\n",
    "ratings = spark.createDataFrame(ratingsRDD_06)\n",
    "ratingsRDD_05 = df_05.rdd.map(lambda p: Row(userId=p[0], itemCol=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],rating=p[-24:]))\n",
    "val = spark.createDataFrame(ratingsRDD_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|             itemCol|              rating|   userId|\n",
      "+--------------------+--------------------+---------+\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|  16132.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1063040.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1063041.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1063042.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1063043.0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5) # 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|             itemCol|              rating|   userId|\n",
      "+--------------------+--------------------+---------+\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1061260.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1061283.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1061284.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1061336.0|\n",
      "|[0, 1, 2, 3, 4, 5...|[0, 0, 1, 0, 0, 0...|1061286.0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val.show(5) # 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 产品与购买情况一一对应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------+\n",
      "| userId|col|              rating|\n",
      "+-------+---+--------------------+\n",
      "|16132.0|  0|[0, 0, 1, 0, 0, 0...|\n",
      "|16132.0|  1|[0, 0, 1, 0, 0, 0...|\n",
      "|16132.0|  2|[0, 0, 1, 0, 0, 0...|\n",
      "|16132.0|  3|[0, 0, 1, 0, 0, 0...|\n",
      "|16132.0|  4|[0, 0, 1, 0, 0, 0...|\n",
      "+-------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---+--------------------+\n",
      "|   userId|col|              rating|\n",
      "+---------+---+--------------------+\n",
      "|1061260.0|  0|[0, 0, 1, 0, 0, 0...|\n",
      "|1061260.0|  1|[0, 0, 1, 0, 0, 0...|\n",
      "|1061260.0|  2|[0, 0, 1, 0, 0, 0...|\n",
      "|1061260.0|  3|[0, 0, 1, 0, 0, 0...|\n",
      "|1061260.0|  4|[0, 0, 1, 0, 0, 0...|\n",
      "+---------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "ratings=ratings.select('userId', explode('itemCol'),'rating')\n",
    "val=val.select('userId', explode('itemCol'),'rating')\n",
    "\n",
    "ratings.show(5)\n",
    "val.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingsRDD = ratings.rdd.map(lambda p: Row(userId=p[0],itemCol=p[1],ranking=p[2][int(p[1])]))\n",
    "ratings2 = ratingsRDD.toDF()\n",
    "ratingsRDD_val = val.rdd.map(lambda p: Row(userId=p[0],itemCol=p[1],ranking=p[2][int(p[1])]))\n",
    "validation = ratingsRDD_val.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|itemCol|ranking| userId|\n",
      "+-------+-------+-------+\n",
      "|      0|      0|16132.0|\n",
      "|      1|      0|16132.0|\n",
      "|      2|      1|16132.0|\n",
      "|      3|      0|16132.0|\n",
      "|      4|      0|16132.0|\n",
      "|      5|      0|16132.0|\n",
      "|      6|      0|16132.0|\n",
      "|      7|      0|16132.0|\n",
      "|      8|      0|16132.0|\n",
      "|      9|      0|16132.0|\n",
      "+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-------+---------+\n",
      "|itemCol|ranking|   userId|\n",
      "+-------+-------+---------+\n",
      "|      0|      0|1061260.0|\n",
      "|      1|      0|1061260.0|\n",
      "|      2|      1|1061260.0|\n",
      "|      3|      0|1061260.0|\n",
      "|      4|      0|1061260.0|\n",
      "|      5|      0|1061260.0|\n",
      "|      6|      0|1061260.0|\n",
      "|      7|      0|1061260.0|\n",
      "|      8|      0|1061260.0|\n",
      "|      9|      0|1061260.0|\n",
      "+-------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings2.show(10)\n",
    "validation.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|itemCol|ranking|userId|\n",
      "+-------+-------+------+\n",
      "|      0|    0.0| 16132|\n",
      "|      1|    0.0| 16132|\n",
      "|      2|    1.0| 16132|\n",
      "|      3|    0.0| 16132|\n",
      "|      4|    0.0| 16132|\n",
      "|      5|    0.0| 16132|\n",
      "|      6|    0.0| 16132|\n",
      "|      7|    0.0| 16132|\n",
      "|      8|    0.0| 16132|\n",
      "|      9|    0.0| 16132|\n",
      "|     10|    0.0| 16132|\n",
      "|     11|    0.0| 16132|\n",
      "|     12|    0.0| 16132|\n",
      "|     13|    0.0| 16132|\n",
      "|     14|    0.0| 16132|\n",
      "|     15|    0.0| 16132|\n",
      "|     16|    0.0| 16132|\n",
      "|     17|    0.0| 16132|\n",
      "|     18|    0.0| 16132|\n",
      "|     19|    0.0| 16132|\n",
      "|     20|    0.0| 16132|\n",
      "|     21|   null| 16132|\n",
      "|     22|   null| 16132|\n",
      "|     23|    0.0| 16132|\n",
      "+-------+-------+------+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training=ratings2.withColumn(\"userId\", ratings2[\"userId\"].cast(\"int\"))\\\n",
    ".withColumn(\"itemCol\", ratings2[\"itemCol\"].cast(\"int\"))\\\n",
    ".withColumn(\"ranking\", ratings2[\"ranking\"].cast(\"double\"))\n",
    "\n",
    "test=validation.withColumn(\"userId\", validation[\"userId\"].cast(\"int\"))\\\n",
    ".withColumn(\"itemCol\", validation[\"itemCol\"].cast(\"int\"))\\\n",
    ".withColumn(\"ranking\", validation[\"ranking\"].cast(\"double\"))\n",
    "\n",
    "training.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 对NULL进行填充："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|itemCol|ranking|userId|\n",
      "+-------+-------+------+\n",
      "|      0|    0.0| 16132|\n",
      "|      1|    0.0| 16132|\n",
      "|      2|    1.0| 16132|\n",
      "|      3|    0.0| 16132|\n",
      "|      4|    0.0| 16132|\n",
      "|      5|    0.0| 16132|\n",
      "|      6|    0.0| 16132|\n",
      "|      7|    0.0| 16132|\n",
      "|      8|    0.0| 16132|\n",
      "|      9|    0.0| 16132|\n",
      "|     10|    0.0| 16132|\n",
      "|     11|    0.0| 16132|\n",
      "|     12|    0.0| 16132|\n",
      "|     13|    0.0| 16132|\n",
      "|     14|    0.0| 16132|\n",
      "|     15|    0.0| 16132|\n",
      "|     16|    0.0| 16132|\n",
      "|     17|    0.0| 16132|\n",
      "|     18|    0.0| 16132|\n",
      "|     19|    0.0| 16132|\n",
      "|     20|    0.0| 16132|\n",
      "|     21|    0.0| 16132|\n",
      "|     22|    0.0| 16132|\n",
      "|     23|    0.0| 16132|\n",
      "+-------+-------+------+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training=training.na.fill(0.0)\n",
    "training.show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|itemCol|ranking| userId|\n",
      "+-------+-------+-------+\n",
      "|      0|    0.0|1061260|\n",
      "|      1|    0.0|1061260|\n",
      "|      2|    1.0|1061260|\n",
      "|      3|    0.0|1061260|\n",
      "|      4|    0.0|1061260|\n",
      "|      5|    0.0|1061260|\n",
      "|      6|    0.0|1061260|\n",
      "|      7|    0.0|1061260|\n",
      "|      8|    0.0|1061260|\n",
      "|      9|    0.0|1061260|\n",
      "|     10|    0.0|1061260|\n",
      "|     11|    0.0|1061260|\n",
      "|     12|    0.0|1061260|\n",
      "|     13|    0.0|1061260|\n",
      "|     14|    0.0|1061260|\n",
      "|     15|    0.0|1061260|\n",
      "|     16|    0.0|1061260|\n",
      "|     17|    0.0|1061260|\n",
      "|     18|    0.0|1061260|\n",
      "|     19|    0.0|1061260|\n",
      "|     20|    0.0|1061260|\n",
      "|     21|    0.0|1061260|\n",
      "|     22|    0.0|1061260|\n",
      "|     23|    0.0|1061260|\n",
      "+-------+-------+-------+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test=test.na.fill(0.0)\n",
    "test.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ALS："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"ranking\", predictionCol=\"prediction\")\n",
    "\n",
    "als = ALS(rank=10, maxIter=10, regParam=0.01, numUserBlocks=10, numItemBlocks=10, implicitPrefs=False, \n",
    "              alpha=1.0, \n",
    "              userCol=\"userId\", itemCol=\"itemCol\", seed=1, ratingCol=\"ranking\", nonnegative=True)\n",
    "\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topredict = test.filter(test['ranking']==0)\n",
    "predictions = model.transform(topredict)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 推荐："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(itemCol=12, ranking=0.0, userId=16339, prediction=0.09994453936815262)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Recommend=predictions.rdd.map(lambda p: Row(user=p[2],ProductPredictions=(p[0],p[3]))).toDF()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-spark]",
   "language": "python",
   "name": "conda-env-jupyter-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
